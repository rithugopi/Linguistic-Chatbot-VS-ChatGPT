{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "dsvTB8u6uQz8",
        "outputId": "21042140-f875-4cc1-b9c0-4e56b95b0dfb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gradio'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f78e58abd0ff>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Download NLTK resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "import gradio as gr\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "class FAQChatbot:\n",
        "    def __init__(self, file_path):\n",
        "        self.data = self.load_data(file_path)\n",
        "        self.stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.word2vec_model = self.train_word2vec()\n",
        "        self.tfidf_vectorizer, self.tfidf_matrix = self.compute_tfidf()\n",
        "\n",
        "    def load_data(self, file_path):\n",
        "        try:\n",
        "            # Load CSV data\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Remove rows with NaN values\n",
        "            df = df.dropna()\n",
        "\n",
        "            # Replace dashes with spaces in recipe_id column\n",
        "            df['recipe_id'] = df['recipe_id'].str.replace('-', ' ')\n",
        "\n",
        "            return df\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"Error: File '{file_path}' not found.\")\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        if text:\n",
        "            words = word_tokenize(text.lower())\n",
        "\n",
        "            # Remove stopwords and lemmatize\n",
        "            words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stop_words]\n",
        "            return words\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def compute_tfidf(self):\n",
        "        tfidf_vectorizer = TfidfVectorizer(tokenizer=self.preprocess_text)\n",
        "        tfidf_matrix = tfidf_vectorizer.fit_transform(self.data['recipe_id'])\n",
        "        return tfidf_vectorizer, tfidf_matrix\n",
        "\n",
        "    def train_word2vec(self):\n",
        "        sentences = [self.preprocess_text(sentence) for sentence in self.data['recipe_id']]\n",
        "        model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "        return model\n",
        "\n",
        "    def find_most_relevant_response(self, input_question):\n",
        "        input_preprocessed = self.preprocess_text(input_question)\n",
        "\n",
        "        # Compute TF-IDF similarity\n",
        "        input_vector = self.tfidf_vectorizer.transform([input_question])\n",
        "        tfidf_similarities = cosine_similarity(input_vector, self.tfidf_matrix)\n",
        "        tfidf_most_similar_index = np.argmax(tfidf_similarities)\n",
        "        tfidf_response = self.data['context_body'].iloc[tfidf_most_similar_index]\n",
        "\n",
        "        # Compute Word2Vec similarity if input words are present in the vocabulary\n",
        "        word2vec_similarities = []\n",
        "        for sentence in self.data['recipe_id']:\n",
        "            sentence_preprocessed = self.preprocess_text(sentence)\n",
        "            if all(token in self.word2vec_model.wv.key_to_index for token in sentence_preprocessed) and all(token in self.word2vec_model.wv.key_to_index for token in input_preprocessed):\n",
        "                input_embedding = np.mean([self.word2vec_model.wv[token] for token in input_preprocessed], axis=0)\n",
        "                sentence_embedding = np.mean([self.word2vec_model.wv[token] for token in sentence_preprocessed], axis=0)\n",
        "                similarity = cosine_similarity(input_embedding.reshape(1, -1), sentence_embedding.reshape(1, -1))[0][0]\n",
        "                word2vec_similarities.append(similarity)\n",
        "            else:\n",
        "                word2vec_similarities.append(-1)  # Placeholder value for words not in vocabulary\n",
        "\n",
        "        word2vec_most_similar_index = np.argmax(word2vec_similarities)\n",
        "        word2vec_response = self.data['context_body'].iloc[word2vec_most_similar_index]\n",
        "\n",
        "        # Decide which response to return based on a combination of both scores\n",
        "        tfidf_weight = 0.9\n",
        "        word2vec_weight = 0.1\n",
        "        combined_score = tfidf_weight * tfidf_similarities[0, tfidf_most_similar_index] + word2vec_weight * word2vec_similarities[word2vec_most_similar_index]\n",
        "        if combined_score >= 0.5:  # Adjust threshold as needed\n",
        "            return tfidf_response\n",
        "        else:\n",
        "            return word2vec_response\n",
        "\n",
        "# Load FAQChatbot\n",
        "faq_chatbot = FAQChatbot('train.csv')\n",
        "\n",
        "def chatbot(input_question):\n",
        "    response = faq_chatbot.find_most_relevant_response(input_question)\n",
        "    return response\n",
        "\n",
        "chatbot_interface = gr.Interface(\n",
        "    fn=chatbot,\n",
        "    inputs=['text'],\n",
        "    outputs=\"text\",\n",
        "    title=\"Advanced FAQ Chatbot\",\n",
        "    description=\"Ask any question related to the provided dataset. This version incorporates multiple techniques to improve accuracy, including TF-IDF and Word2Vec embeddings.\",\n",
        "    examples=[\n",
        "        [\"What is the recipe for caramel dumplings?\"],\n",
        "        [\"How do I make zucchini bread?\"],\n",
        "        [\"How to make zucchini pizza?\"],\n",
        "\n",
        "    ],\n",
        "    allow_flagging = False,\n",
        "    theme=\"black\"\n",
        ")\n",
        "\n",
        "chatbot_interface.launch(share = True)\n"
      ]
    }
  ]
}